{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this assignment will be **automatically graded**. Please take note of the following:\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- You can add additional cells, but it is not recommended to (re)move cells. Cells required for autograding cannot be moved and cells containing tests cannot be edited.\n",
    "- You are allowed to use a service such as [Google Colaboratory](https://colab.research.google.com/) to work together. However, you **cannot** hand in the notebook that was hosted on Google Colaboratory, but you need to copy your answers into the original notebook and verify that it runs succesfully offline. This is because Google Colaboratory destroys the metadata required for grading.\n",
    "- Name your notebook **exactly** `{TA_name}_{student1_id}_{student2_id}_lab{i}.ipynb`, for example `wouter_12345_67890_lab1.ipynb` (or tim|elise|david|qi, depending on your TA), **otherwise your submission will be skipped by our regex and you will get 0 points** (but no penalty as we cannot parse your student ids ;)).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = \"David Speck, Victor Zuanazzi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4eed621d3748a44866956caa0de5247b",
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bab7b3976d6730a0739fd462766b1d42",
     "grade": false,
     "grade_id": "cell-9ebb0d5b306dbdea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Policy Evaluation (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8d010aef9b5b288e694006a2aefe67e0",
     "grade": false,
     "grade_id": "cell-1078e8f0b90517ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this exercise we will evaluate a policy, e.g. find the value function for a policy. The problem we consider is the gridworld from Example 4.1 in the book. The environment is implemented as `GridworldEnv`, which is a subclass of the `Env` class from [OpenAI Gym](https://github.com/openai/gym). This means that we can interact with the environment. We can look at the documentation to see how we can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "990081b68602e7e0c46f2edeab0fcb53",
     "grade": false,
     "grade_id": "cell-de586c5ac92d8d74",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()\n",
    "# Lets see what this is\n",
    "?env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "814f4db75653991276d29ebff9d6ae37",
     "grade": false,
     "grade_id": "cell-b3a84dfb0e66a0c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# To have a quick look into the code\n",
    "??env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe850a3b9a1be42ae79b895d206ac3b6",
     "grade": false,
     "grade_id": "cell-b2162d776f0c2014",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we want to evaluate a policy by using Dynamic Programming. For more information, see the [Intro to RL](https://drive.google.com/open?id=1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG) book, section 4.1. This algorithm requires knowledge of the problem dynamics in the form of the transition probabilities $p(s',r|s,a)$. In general these are not available, but for our gridworld we know the dynamics and these can be accessed as `env.P`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a moment to figure out what P represents. \n",
    "# Note that this is a deterministic environment. \n",
    "# What would a stochastic environment look like?\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d2d2b829d45d264cf8a6194dc8ccc132",
     "grade": false,
     "grade_id": "cell-209a484040bd874f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    Implementation of Iterative Policy Evaluation, Chapter 4, pag 75 from Sutton\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    max_delta = theta + 1  # Arbitrary initialization to enter the evaluation loop.\n",
    "    \n",
    "    while max_delta > theta:\n",
    "        \n",
    "        # Resets max_delta to store the max change of the current iteration.\n",
    "        max_delta = 0  \n",
    "        \n",
    "        # Evaluation over all states\n",
    "        for state in range(env.nS):\n",
    "            new_v = 0\n",
    "            # Evaluation over all possible actions\n",
    "            for action, action_prob in enumerate(policy[state]):\n",
    "                \n",
    "                # Each action leads may lead to several next states\n",
    "                for  trans_prob, next_state, reward, done in env.P[state][action]:\n",
    "                    \n",
    "                    new_v += action_prob * trans_prob * (reward + discount_factor * V[next_state])\n",
    "            \n",
    "            delta = abs(new_v - V[state])\n",
    "            V[state] = new_v  # Updates the state value.\n",
    "            max_delta = delta if delta > max_delta else max_delta  # Refreshes the stoping criteria.\n",
    "\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run your code, does it make sense?\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval(random_policy, env)\n",
    "V \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5d879d65fc89af254883e1b68234e76e",
     "grade": true,
     "grade_id": "cell-b5c9d69b1731aff5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test: When you hand in the nodebook we will check that the value function is (approximately) what we expected\n",
    "# but we need to make sure it is at least of the correct shape\n",
    "v = policy_eval(random_policy, env)\n",
    "assert v.shape == (env.nS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "863ed58baecbbb4923162f40084e870d",
     "grade": false,
     "grade_id": "cell-b680e98c9ff204b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Policy Iteration (2 points)\n",
    "Using the policy evaluation algorithm we can implement policy iteration to find a good policy for this problem. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cfa494b2b437f9007f6b29b1ed5e0f78",
     "grade": false,
     "grade_id": "cell-383c54749617512c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_Q(state, V, discount_factor):\n",
    "    \"\"\"Calculates the Q(s, a) values for a state.\n",
    "    \n",
    "    Args:\n",
    "        state: (int) the index of the state which Q values are of interest.\n",
    "        V: (np.array) estimated V values.\n",
    "        discount_factor: (float <=1) the discouting factor for the reward of future states.\n",
    "        \n",
    "    Returns:\n",
    "        (np.array of shape env.nA) with the estimated Q values.\"\"\"\n",
    "    \n",
    "    Q = np.zeros(env.nA)\n",
    "    \n",
    "    for action in range(env.nA):\n",
    "        for trans_prob, next_state, reward, done in env.P[state][action]:\n",
    "            Q[action] += trans_prob * (reward + discount_factor * V[next_state])\n",
    "            \n",
    "    return Q\n",
    "\n",
    "def policy_improvement(env, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Chapter 4, pag 80 from Sutton\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    while not converged:\n",
    "        \n",
    "        V = policy_eval(policy, env, discount_factor)\n",
    "        \n",
    "        converged = True\n",
    "        \n",
    "        for state in range(env.nS):\n",
    "            \n",
    "            # get the greedy action for the curent state \n",
    "            # under the current policy\n",
    "            greedy_action = np.argmax(policy[state])\n",
    "            \n",
    "            Q = calc_Q(state, V, discount_factor)\n",
    "            \n",
    "            # get the best action for the curent state \n",
    "            # given the current estimations of Q(a | s)\n",
    "            best_action = np.argmax(Q)\n",
    "            \n",
    "            policy[state] = np.eye(env.nA)[best_action]\n",
    "            \n",
    "            # If nothing changes, the while-loop is exited.\n",
    "            if greedy_action != best_action:\n",
    "                converged = False\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c4ab9c8d01a5902c276a3fbfbcc89e01",
     "grade": true,
     "grade_id": "cell-8c62e92d1f34720b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_improvement(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eee712b931eb830cb89792ef30675558",
     "grade": true,
     "grade_id": "cell-695dc14dbc6a8f95",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is not an empty cell. It is needed for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c926edcbca292c79812c5b27eab63108",
     "grade": false,
     "grade_id": "cell-ceb755afcff43612",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3. Value Iteration (3 points)\n",
    "Now implement the value iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "87f515e22f7ad0ea461271479dff3f5e",
     "grade": false,
     "grade_id": "cell-574fc5f6932fa4cc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    Implementation of the algorithm as described at Sutto book Ch 4, pag 83.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    # Arbitrary initialization to enter the while loop.\n",
    "    max_delta = theta + 1\n",
    "    \n",
    "    while max_delta > theta:\n",
    "        \n",
    "        max_delta = 0\n",
    "        \n",
    "        # Calculates the Value of all states V[:]\n",
    "        for state in range(env.nS):\n",
    "            \n",
    "            # Get the Q(a | s) \n",
    "            Q = calc_Q(state, V, discount_factor)\n",
    "            \n",
    "            # get the value of the action with maximum value.\n",
    "            v_best = np.max(Q)\n",
    "            \n",
    "            # necessary for the while-loop exit condition\n",
    "            delta = np.abs(v_best - V[state])\n",
    "            \n",
    "            # the value of the state is the same as \n",
    "            # the value of the best action.\n",
    "            V[state] = v_best      \n",
    "            \n",
    "        # Refreshes the stoping criteria.\n",
    "        max_delta = delta if delta > max_delta else max_delta \n",
    "        \n",
    "    # Deterministic greedy policy:\n",
    "    # Is there a way of doing it without a loop?\n",
    "    for state in range(env.nS):\n",
    "\n",
    "        # Find the greedy action.\n",
    "        Q = calc_Q(state, V, discount_factor)\n",
    "        best_action = np.argmax(Q)\n",
    "        \n",
    "        # Set the probability of the greedy action to 1.\n",
    "        policy[state, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d103427f5b98a8957ad486243f98e64c",
     "grade": true,
     "grade_id": "cell-b82ed3adfeecc757",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Oh let's test again\n",
    "# Let's see what it does\n",
    "policy, v = value_iteration(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3add7d8b6101d0e3b6250b6bb064566c",
     "grade": false,
     "grade_id": "cell-ded21ac846e244a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What is the difference between value iteration and policy iteration? Which algorithm is most efficient (e.g. needs to perform the least *backup* operations)? Please answer *concisely* in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "078f713af4c6bf3af8fb31b8da772758",
     "grade": true,
     "grade_id": "cell-940a8d8e21f18f69",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Policy Iteration requires a policy evaluation step at each iteration, which is itself iteractive. Value iteration goes around it by only performing one sweep of the policy evaluation. Value Iteration is also guaranteed to converge under the same conditions of Policy Iteration.\n",
    "\n",
    "Value Iteration is more efficiently because it requires a fewer number of computations per iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98ec4e85c09c116f6fe1658fa0451e33",
     "grade": false,
     "grade_id": "cell-7ab207a9f93cf4d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4. Monte Carlo Prediction (7 points)\n",
    "What is the difference between Dynamic Programming and Monte Carlo? When would you use the one or the other algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dba98a584a2d9c97735f96547ac7442a",
     "grade": true,
     "grade_id": "cell-74a904ed87b8e2cc",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Dynamic Programming looks at all actions and their resulting states to update the value of the current state. Monte Carlo on the otherside considers at a full episode to update the value of a state.\n",
    "Dynamic Programming requires to have a model then tells whichs state one ends up in after performing an action in the current state. Monte Carlo does not require that model however it is limited to episodic problems since it only updates values after completing an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d826feda7dc9cab51ad9db8ccbdfadf",
     "grade": false,
     "grade_id": "cell-5f0c1d608436b67b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the Monte Carlo Prediction we will look at the Blackjack game (Example 5.1 from the book), for which the `BlackjackEnv` is implemented in `blackjack.py`. Note that compared to the gridworld, the state is no longer a single integer, which is why we use a dictionary to represent the value function instead of a numpy array. By using `defaultdict`, each state gets a default value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb7a884505c5973aff2fe9998cc104e2",
     "grade": false,
     "grade_id": "cell-a342b69fcfdea5b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from blackjack import BlackjackEnv\n",
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26ae90f402b5de7d07f23e776a525c4b",
     "grade": false,
     "grade_id": "cell-7366692dee80c32c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the Monte Carlo algorithm, we need to *interact* with the environment. This means that we start an episode by using `env.reset` and send the environment actions via `env.step` to observe the reward and next observation (state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef5fc8c121d400f2192646c9201e7769",
     "grade": false,
     "grade_id": "cell-85356add2643980e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# So let's have a look at what we can do in general with an environment...\n",
    "import gym\n",
    "?gym.Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0433c9161cddc2d9171c8e87b1e9b444",
     "grade": false,
     "grade_id": "cell-251b7b17c5d08a24",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We can also look at the documentation/implementation of a method\n",
    "?env.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42aceb4ea182a1aacfb40c5f201d4b01",
     "grade": false,
     "grade_id": "cell-6decb2ab83c5bcec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "??BlackjackEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8fa896a9ea64567134c861511cfa6011",
     "grade": false,
     "grade_id": "cell-ae161126d3cb1b7b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "A very simple policy for Blackjack is to *stick* if we have 20 or 21 points and *hit* otherwise. We want to know how good this policy is. This policy is *deterministic* and therefore a function that maps an observation to a single action. Technically, we can implement this as a dictionary or as a function, where we use the latter. To get started, let's implement this simple policy for BlackJack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc1b192852416b81f96f0858f1389d67",
     "grade": false,
     "grade_id": "cell-9fdcb503df9cdb08",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_policy(observation):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is >= 20 and hits otherwise.\n",
    "    \"\"\"\n",
    "    sum_hand = observation[0]\n",
    "    return sum_hand < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bf04382f012b102f2c5c360bb8da3241",
     "grade": true,
     "grade_id": "cell-99f02e2d9b338a5b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "print(s)\n",
    "a = simple_policy(s)\n",
    "print(env.step(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b5e33f6499dd4040071dc62a3e8fccb2",
     "grade": false,
     "grade_id": "cell-0184f4c719afb98c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement either the MC prediction algorithm (either first visit or every visit). Hint: you can use `for i in tqdm(range(num_episodes))` to show a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a90a42914d0f7ade801bee9a8bd04e19",
     "grade": true,
     "grade_id": "cell-b822e9d13cf1f65e",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final value function\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        # Generate episode\n",
    "        done = False\n",
    "        episode = []\n",
    "        while not done:\n",
    "            s = env.reset()\n",
    "            a = policy(s)\n",
    "            new_s, reward, done, _ = env.step(a)\n",
    "            episode.append((s, a, reward))\n",
    "        \n",
    "        G = 0\n",
    "        for i, (s, a, r) in enumerate(episode[::-1]):\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # check if it first visit <=> no earlier step with same state\n",
    "            earlier_steps = list(filter(lambda step: step[0] == 1,episode[:-i]))\n",
    "            first_visit = len(earlier_steps) == 0\n",
    "            \n",
    "            if first_visit:\n",
    "                returns_sum[s] = returns_sum[s] + G\n",
    "                returns_count[s] = returns_count[s] + 1\n",
    "                V[s] = returns_sum[s] / returns_count[s]\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = mc_prediction(simple_policy, env, num_episodes=1000)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "65fb9d8060a4c843e72474169396eb3e",
     "grade": false,
     "grade_id": "cell-9d32f907f180c088",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now make *4 plots* like Figure 5.1 in the book. You can either make 3D plots or heatmaps. Make sure that your results look similar to the results in the book. Give your plots appropriate titles, axis labels, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a4eee824f088a6c13ee8c9296af4561",
     "grade": false,
     "grade_id": "cell-cbaf4d6a0e4c00fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's run your code one time\n",
    "V_10k = mc_prediction(simple_policy, env, num_episodes=10000)\n",
    "V_500k = mc_prediction(simple_policy, env, num_episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "84102d69cdad07cf0100d846346b65e6",
     "grade": true,
     "grade_id": "cell-ba046443478aa517",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dealer value range\n",
    "d_range = range(1, 11)\n",
    "# player value range\n",
    "p_range = range(12, 22)\n",
    "# usable ace range\n",
    "ace_range = [True, False]\n",
    "\n",
    "X = np.array([list(d_range)] * len(p_range)).T\n",
    "Y = np.array([list(p_range)] * len(d_range))\n",
    "Zs = np.zeros((4, X.shape[0], X.shape[1]))\n",
    "titles = ['10k episodes, usable ace', '500k episodes, usable ace', '10k episodes, no ace', '500k episodes, no ace']\n",
    "\n",
    "for i, d in enumerate(d_range):\n",
    "    for j, p in enumerate(p_range):\n",
    "        Zs[0,i,j] = V_10k[(p, d, True)]\n",
    "        Zs[1,i,j] = V_500k[(p, d, True)]\n",
    "        Zs[2,i,j] = V_10k[(p, d, False)]\n",
    "        Zs[3,i,j] = V_500k[(p, d, False)]\n",
    "            \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
    "    ax.plot_wireframe(X, Y, Zs[i])\n",
    "    ax.set_zlim(-1,1)\n",
    "    ax.set_zticks([-1,1])\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('dealer showing')\n",
    "    ax.set_ylabel('player sum')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c1baa558731a1618d32691224e5b5780",
     "grade": false,
     "grade_id": "cell-a5cc039e3d648855",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5. Monte Carlo control with $\\epsilon$-greedy policy (5 points)\n",
    "Now we have a method to evaluate state-values given a policy. Take a moment to think whether we can use the value function to find a better policy? Assuming we do not know the dynamics of the environment, why is this not possible?\n",
    "\n",
    "We want a policy that selects _actions_ with maximum value, e.g. is _greedy_ with respect to the _action-value_ (or Q-value) function $Q(s,a)$. We need to keep exploring, so with probability $\\epsilon$ we will take a random action. First, lets implement a function `make_epsilon_greedy_policy` that takes the Q-value function and returns an $\\epsilon$-greedy policy. The policy itself is a function that returns an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f1469d08ccd4665219d42e03b709c690",
     "grade": true,
     "grade_id": "cell-78eff3f4ca0f0e09",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        an action according to the epsilon-greedy policy.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        \n",
    "        # All policies receive epsilon / nA probability\n",
    "        p_all = np.array([epsilon / nA] * nA)\n",
    "        \n",
    "        # Define the probability to be added to the greedy action\n",
    "        p_greedy = 1 - epsilon\n",
    "        \n",
    "        # Finds the action with max Q value\n",
    "        greedy_idx = np.argmax(Q[observation])\n",
    "        \n",
    "        p_all[greedy_idx] += p_greedy\n",
    "        \n",
    "        assert np.sum(p_all) == 1.0\n",
    "        \n",
    "        return p_all\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "12a7f4ab3c649a1d768ce80d4573cf28",
     "grade": true,
     "grade_id": "cell-2fc0baa87f31ab98",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Again, keep track of counts for efficiency\n",
    "    # returns_sum, returns_count and Q are \n",
    "    # nested dictionaries that map state -> (action -> action-value).\n",
    "    # We could also use tuples (s, a) as keys in a 1d dictionary, but this\n",
    "    # way Q is in the format that works with make_epsilon_greedy_policy\n",
    "    \n",
    "    # Not sure what to do with that!\n",
    "#     returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "#     returns_count = defaultdict(lambda: np.zeros(env.action_space.n, dtype=int))\n",
    "    \n",
    "    # The final action-value function.\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    returns_SA = {}\n",
    "\n",
    "    max_steps = 10\n",
    "    \n",
    "    for ep in tqdm(range(num_episodes)):\n",
    "        \n",
    "        episode = []  # (state, action, reward, first_visit)\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Necessary?\n",
    "        unique_SA = set() # tracks the (state, action)\n",
    "        \n",
    "        # Set the stopying criteria.\n",
    "        done = False\n",
    "        step = max_steps  # Avoids episodes that are too long.\n",
    "        \n",
    "        # Generate an episode.\n",
    "        while not done:\n",
    "            \n",
    "            # defines an action\n",
    "            p_all = policy(state)\n",
    "            action = np.random.choice(np.arange(len(p_all)), p=p_all)\n",
    "            \n",
    "            # tracks if this state was visited before\n",
    "            first_visit = True\n",
    "            if (state, action) in unique_SA:\n",
    "                first_visit = False\n",
    "            else:\n",
    "                unique_SA.add((state, action))\n",
    "            \n",
    "            # transitions to a next state, given the chosen action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward, first_visit))\n",
    "            state = next_state\n",
    "            \n",
    "            # Safety stop criteria\n",
    "            step -= 1\n",
    "            if step <= 0:\n",
    "                break\n",
    "        \n",
    "        # Initial total reward of the episode is zero.\n",
    "        G = 0 \n",
    "        \n",
    "        for idx, (state, action, reward, first_visit) in enumerate(reversed(episode)):\n",
    "            sa_pair = (state, action)\n",
    "            \n",
    "            # Calculate the expected reward of (state, action)\n",
    "            G = discount_factor * G + reward\n",
    "            \n",
    "            if first_visit:\n",
    "                # average reward of (state, action), number of visits so far\n",
    "                q_sa, n_iter = returns_SA.get(sa_pair, (0, 0))\n",
    "                n_iter += 1  # Updates the number of times this (state, action) was encontered\n",
    "\n",
    "                # Iteratively updates the averages\n",
    "                returns_SA[sa_pair] = (q_sa + (G - q_sa) / n_iter, n_iter)\n",
    "\n",
    "                Q[state][action] = returns_SA[sa_pair][0]\n",
    "            else:\n",
    "                print(f\"another visit: {idx}, {state}, {action}, {reward}, {G}\")\n",
    "\n",
    "    return Q, policy\n",
    "    \n",
    "# Test it quickly\n",
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=10000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9ea50e12589ce430405fa2e92ee0c108",
     "grade": false,
     "grade_id": "cell-e6170d8979ca2a9c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "94d3f0aecbe1b5ebfed94e7b1379f617",
     "grade": false,
     "grade_id": "cell-449e36eb98369942",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "How can you obtain the (V-)value function from the Q-value function? Plot the (V-)value function that is the result of 500K iterations. Additionally, visualize the greedy policy similar to Figure 5.2 in the book. Use a white square for hitting, black for sticking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ef932ff88d976ffefc0338822d7d2af9",
     "grade": true,
     "grade_id": "cell-7d797248a3b132f5",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Copied from https://github.com/dennybritz/reinforcement-learning/blob/master/lib/plotting.py\n",
    "\n",
    "def get_Z(X, Y, use_ace=True):\n",
    "    return np.apply_along_axis(lambda x: V[(x[0], x[1], use_ace)], 2, np.dstack([X, Y]))\n",
    "\n",
    "def color_from_action(X, Y, A, use_ace=True):\n",
    "    c = np.apply_along_axis(lambda x: A[(x[0], x[1], use_ace)], 2, np.dstack([X, Y]))\n",
    "    \n",
    "    return c.astype(str)\n",
    "\n",
    "def blackjack_plot(V, A, use_ace=True, title=\"Value Function\"):\n",
    "    \"\"\"\n",
    "    Plots the value function as a surface plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # X axis keeps the sum of the player\n",
    "    x_range = np.arange(min(k[0] for k in V.keys()), max(k[0] for k in V.keys()))\n",
    "    \n",
    "    # Y axis keeps the sum of the Dealer\n",
    "    y_range = np.arange(min(k[1] for k in V.keys()), max(k[1] for k in V.keys()))\n",
    "    \n",
    "    # Define a meshgrid\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "    # Find value for all (x, y) coordinates\n",
    "    Z = get_Z(X, Y, use_ace)\n",
    "    \n",
    "    a_colors = color_from_action(X, Y, A, use_ace)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    surf = ax.plot_surface(X, Y, Z, facecolors= a_colors , vmin=-1.0, vmax=1.0, shade=False) # plt.cm.jet(a_colors)\n",
    "    surf = ax.plot_wireframe(X, Y, Z,)\n",
    "    \n",
    "    ax.set_xlabel('Player Sum')\n",
    "    ax.set_ylabel('Dealer Showing')\n",
    "    ax.set_zlabel('Value')\n",
    "    ax.set_title(title)\n",
    "    ax.view_init(ax.elev, 120)\n",
    "    plt.show()\n",
    "\n",
    "V = defaultdict(float)\n",
    "A = defaultdict(float)\n",
    "\n",
    "# We only care for the greedy action of each state.\n",
    "for state in Q:\n",
    "    action = Q[state]\n",
    "    V[state] = np.max(action)  # get the value of the greedy action.\n",
    "    A[state] = np.argmax(action)  # get the greedy action.\n",
    "\n",
    "blackjack_plot(V, A, use_ace=True, title=\"Greedy Value Function (Usable Ace)\")\n",
    "blackjack_plot(V, A, use_ace=False, title=\"Greedy Value Function (No Usable Ace)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
